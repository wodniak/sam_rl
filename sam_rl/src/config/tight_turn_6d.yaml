###### Training hyperparameters
episode_length: 400  # 1000: 10s of sim flight per episode
total_episodes: 5000
num_cpu: 1
save_freq: 100  # episodes
eval_freq: 10  # episodes
n_eval_episodes: 5  # episodes

###### Testing hyperparameters
test_episode_length: 3000
test_env_dt: 0.01
test_setpoints: [
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
]

###### Environment state/actions
env_dt: 0.2
env_reward_fn_type: "tight_turn"
# define normalization
env_use_vecnormalize: True
vecnormalize:
    training: True
    norm_obs: True
    norm_reward: True
    clip_obs: 40.0
    gamma: 0.99
# define observed state and its max values
env_state:
    x: 200
    y: 200
    psi: 10
    u: 30
    v: 30
    r: 30
# reset at random uniformly each episode, given max values. Other vars are 0's
env_state_reset:
    psi:
        min: -2.3
        max: -2.3
    x:
        min: 2.
        max: 2.
    y:
        min: 2.
        max: 2.

# define available actions and its position in action_6d vector
env_actions:
    rpm: 1
    dr: 3

###### Q R R_r weight matrices for reward function
# define Q
env_state_weights_Q:
    x: 0.5
    y: 0.5
    z: 2.
    phi: 0.0
    theta: 0.0
    psi: 0.0
    u: 0.3
    v: 0.3
    w: 0.3
    p: 0.3
    q: 3.0
    r: 0.3
# define R
env_actions_weights_R:
    rpm: 0.03
    de: 0.03
    dr: 0.03
    lcg: 0.03
    vbs: 0.03
# define R_r
env_actions_weights_R_r:
    rpm: 0.3
    de: 0.3
    dr: 0.3
    lcg: 0.3
    vbs: 0.3

###### Network architecture
# for off-policy only
off_policy_kwargs:
    net_arch:
        pi:
        - 64
        - 64
        qf:
        - 64
        - 64
# for on-policy only
on_policy_kwargs:
    net_arch: [
        pi: [64, 64],
        qf: [64, 64]
    ]

###### Network hyperparameters
learning_rate: 0.001
buffer_size: 1000000
learning_starts: 128
batch_size: 64
tau: 0.005
gamma: 0.99
train_freq: 5  # episodes
gradient_steps: -1  # all accumulated
sigma: 0.1  # action noise

###### Misc
device: "auto"
verbose:  0
tensorboard_log: null
model_dir: null
